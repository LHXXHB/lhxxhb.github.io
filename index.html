<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>dapeng_hu.info</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16x16.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Dapeng Hu</name>
              </p>
              <p>
                I'm a research scientist at <a href="https://www.a-star.edu.sg/cfar">A*STAR's Centre for Frontier AI Research (CFAR)</a>.
                I recently completed my Ph.D. at the <a href="https://cde.nus.edu.sg/ece/">National University of Singapore (NUS)</a>, specializing in transfer learning and representation learning for computer vision.
                Before my Ph.D., I graduated with distinction in electronic engineering from <a href="https://www.nju.edu.cn/EN/main.htm">Nanjing University</a>.
              </p>

              <p style="text-align:center">
                <a href="mailto:lhxxhb15@gmail.com">Email</a> &nbsp/&nbsp
                <!-- next paper <a href="data/dapeng_CV.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=wv9HjA0AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/dapeng-hu-389bb5134/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%;border-radius:50%;" alt="profile photo" src="favicon/avatar.jpg" class="hoverZoomLink"/>
            </td>
          </tr>
            </tbody></table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    My research interests lie in computer vision and deep learning. Specifically, I'm interested in generalizable and label-efficient deep learning, covering but not limited to the following topics:
                  </p>
                  <ul>
                    <li> Domain adaptation and semi-supervised learning: How to leverage unlabeled data for training a task-specific model? 
                    <li> Representation learning and self-supervised learning: How to pre-train a generalized foundation model?
                    <li> Model-based transfer learning and fine-tuning: How to transfer the knowledge in a pre-trained source model or foundation model to a different domain or task? 
                    <li> Large-scale empirical studies on deep learning models: How to observe, understand, and explain the behavior of foundation models? 
                  </ul>
                </td>
              </tr>
            </tbody></table>
<!-- paper list begins -->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="two" id='impact_image'><img src='images/mixval.png'></div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Mixed Samples as Probes for Unsupervised Model Selection in Domain Adaptation</papertitle> 
        <br>
            <strong>Dapeng Hu</strong>,	
            <a href="https://scholar.google.com/citations?user=eY8i-mQAAAAJ&hl=en">Jian Liang</a>,	
            <a href="https://scholar.google.com.sg/citations?user=8gm-CYYAAAAJ&hl=en">Jun Hao Liew</a>,	
            <a href="https://scholar.google.com.sg/citations?user=KJU5YRYAAAAJ&hl=en">Chuhui Xue</a>,	
            <a href="https://scholar.google.com/citations?user=LXuWMF4AAAAJ&hl=en">Song Bai</a>,
            <a href="https://scholar.google.com/citations?hl=en&user=w69Buq0AAAAJ">Xinchao Wang</a>

        <br>
            <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023.
        <br>
            <a target="_blank">arXiv</a>
        <br>
        <br>
            <p></p>
            <p> We proposed MixVal, a novel target-only validation method for unsupervised domain adaptation with state-of-the-art model selection performance and improved stability.</p>
        </td>
    </tr>
    </tbody></table>
    <!-- next paper -->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="two" id='impact_image'><img src='images/pseudocal.png'></div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>PseudoCal: A Source-Free Approach to Unsupervised Uncertainty Calibration in Domain Adaptation</papertitle>
        <br>
            <strong>Dapeng Hu</strong>,
            <a href="https://scholar.google.com/citations?user=eY8i-mQAAAAJ&hl=en">Jian Liang</a>,
            <a href="https://scholar.google.com/citations?hl=en&user=w69Buq0AAAAJ">Xinchao Wang</a>,	
            <a href="https://scholar.google.com/citations?user=AgbeqGkAAAAJ&hl=en">Chuan-Sheng Foo<sup>&#8224</sup></a>	

        <br>
            <em>Under review</em>
        <br>
            <a href="https://arxiv.org/pdf/2307.07489.pdf" target="_blank">arXiv</a>
        <br>
        <br>
            <p></p>
            <p>We proposed PseudoCal, a source-free calibration method for unsupervised domain adaptation, which outperforms existing methods in reducing calibration error across 10 UDA methods.</p>
        </td>
    </tr>
    </tbody></table>
<!-- next paper -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="two" id='impact_image'><img src='images/umad.png'></div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>UMAD: Universal Model Adaptation under Domain and Category Shift</papertitle>
        <br>
            <a href="https://scholar.google.com/citations?user=eY8i-mQAAAAJ&hl=en">Jian Liang*</a>,
            <strong>Dapeng Hu*</strong>,
            <a href="https://scholar.google.com/citations?user=Q8iay0gAAAAJ&hl=en">Jiashi Feng</a>,	
            <a href="https://scholar.google.com/citations?user=ayrg9AUAAAAJ&hl=en">Ran He</a>	

        <br>
            <em>Under review</em>
        <br>
            <a href="https://arxiv.org/pdf/2112.08553.pdf" target="_blank">arXiv</a>
        <br>
        <br>
            <p></p>
            <p>We proposed a novel and effective method UMAD to tackle realistic open-set domain adaptation tasks where neither source data nor the prior about the label set overlap across domains is available for target domain adaptation.</p>
        </td>
    </tr>
    </tbody></table>
<!-- next paper -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="two" id='impact_image'><img src='images/dine.png'></div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
            <a href=https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_DINE_Domain_Adaptation_From_Single_and_Multiple_Black-Box_Predictors_CVPR_2022_paper.pdf>
            <papertitle>DINE: Domain Adaptation from Single and Multiple Black-box Predictors</papertitle> </a>
        <br>
            <a href="https://scholar.google.com/citations?user=eY8i-mQAAAAJ&hl=en">Jian Liang</a>,
            <strong>Dapeng Hu</strong>,
            <a href="https://scholar.google.com/citations?user=Q8iay0gAAAAJ&hl=en">Jiashi Feng</a>,	
            <a href="https://scholar.google.com/citations?user=ayrg9AUAAAAJ&hl=en">Ran He</a>	

        <br>
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022. <strong>Oral</strong>
        <br>
            <a href="https://arxiv.org/pdf/2104.01539.pdf" target="_blank">arXiv</a> &nbsp/&nbsp
            <a href="https://github.com/tim-learn/DINE" target="_blank">code</a>
        <br>
        <br>
            <p></p>
            <p>We studied a realistic and challenging domain adaptation problem and proposed a safe and efficient adaptation framework (DINE) with only black-box predictors provided from source domains.</p>
        </td>
    </tr>
    </tbody></table>
<!-- next paper -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="two" id='impact_image'><img src='images/cssl.png'></div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <a href=https://openreview.net/pdf?id=EwqEx5ipbOu>
              <papertitle>How Well Does Self-Supervised Pre-Training Perform with Streaming Data?</papertitle> </a>
          <br>
              <strong>Dapeng Hu*</strong>,
              <a href="https://scholar.google.com/citations?hl=en&user=oYILsyoAAAAJ">Shipeng Yan*</a>,	
              <a href="https://openreview.net/profile?id=~Qizhengqiu_Lu1">Qizhengqiu Lu</a>,	
              <a href="https://scholar.google.com/citations?hl=en&user=2p7x6OUAAAAJ">Lanqing Hong<sup>&#8224</sup></a>,	
              <a href="https://scholar.google.com/citations?hl=en&user=rvYUgBwAAAAJ">Hailin Hu</a>,	
              <a href="https://scholar.google.com/citations?hl=en&user=zuYIUJEAAAAJ">Yifan Zhang</a>,	
              <a href="https://scholar.google.com/citations?hl=en&user=XboZC1AAAAAJ">Zhenguo Li</a>,	
              <a href="https://scholar.google.com/citations?hl=en&user=w69Buq0AAAAJ">Xinchao Wang<sup>&#8224</sup></a>,	
              <a href="https://scholar.google.com/citations?user=Q8iay0gAAAAJ&hl=en">Jiashi Feng</a>

          <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2022.
          <br>
              <a href="https://arxiv.org/pdf/2104.12081.pdf" target="_blank">arXiv</a>
          <br>
          <br>
              <p></p>
              <p>We conducted the first thorough empirical evaluation to investigate how well self-supervised learning (SSL) performs with various streaming data types and diverse downstream tasks.</p>
          </td>
      </tr>
      </tbody></table>
<!-- next paper -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="two" id='impact_image'><img src='images/noun.png'></div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <a href=https://ieeexplore.ieee.org/document/9611076>
              <papertitle>Adversarial Domain Adaptation With Prototype-Based Normalized Output Conditioner</papertitle> </a>
          <br>
              <strong>Dapeng Hu</strong>,
              <a href="https://scholar.google.com/citations?user=eY8i-mQAAAAJ&hl=en">Jian Liang<sup>&#8224</sup></a>,
              <a href="https://scholar.google.com/citations?hl=en&user=xRz9vN4AAAAJ">Hanshu Yan</a>,	
              <a href="https://scholar.google.com/citations?hl=en&user=fF8OFV8AAAAJ">Qibin Hou</a>,	
              <a href="https://scholar.google.com/citations?hl=en&user=-JiwekUAAAAJ">Yunpeng Chen</a>	

          <br>
              <em>IEEE Transactions on Image Processing (TIP)</em>, Volume 30, 2021.
          <br>
              <a href="https://arxiv.org/pdf/2003.13274.pdf" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href="https://github.com/tim-learn/NOUN" target="_blank">code</a>
          <br>
          <br>
              <p></p>
              <p>We proposed a novel, efficient, and generic conditional domain adversarial training method NOUN to solve domain adaptation tasks for both image classification and segmentation. </p>
          </td>
      </tr>
      </tbody></table>
<!-- next paper -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="two" id='impact_image'><img src='images/shot++.png'></div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <a href=https://ieeexplore.ieee.org/document/9512429>
              <papertitle>Source Data-absent Unsupervised Domain Adaptation through Hypothesis Transfer and Labeling Transfer</papertitle> </a>
          <br>
              <a href="https://scholar.google.com/citations?user=eY8i-mQAAAAJ&hl=en">Jian Liang</a>,
              <strong>Dapeng Hu</strong>,
              <a href="http://59.108.48.34/tiki/news/news.php?id=EGwangyunbo">Yunbo Wang</a>,
              <a href="https://scholar.google.com/citations?user=ayrg9AUAAAAJ&hl=en">Ran He</a>,	
              <a href="https://scholar.google.com/citations?user=Q8iay0gAAAAJ&hl=en">Jiashi Feng</a>

          <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2021.
          <br>
              <a href="https://arxiv.org/pdf/2012.07297.pdf" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href="https://github.com/tim-learn/SHOT-plus" target="_blank">code</a>
          <br>
          <br>
              <p></p>
              <p>We enhanced SHOT to SHOT++ with an intra-domain labeling transfer strategy and achieved even better source data-free adaptation results than state-of-the-art data-dependent results. </p>
          </td>
      </tr>
      </tbody></table>
<!-- next paper -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="two" id='impact_image'><img src='images/ccvr.png'></div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <a href=https://proceedings.neurips.cc/paper/2021/hash/2f2b265625d76a6704b08093c652fd79-Abstract.html>
              <papertitle>No Fear of Heterogeneity: Classifier Calibration for Federated Learning with Non-IID Data</papertitle> </a>
          <br>
              <a href="https://scholar.google.com/citations?user=eL-xIlAAAAAJ&hl=en">Mi Luo</a>,	
              <a href="https://scholar.google.com/citations?hl=en&user=LhjKgcoAAAAJ">Fei Chen</a>,	
              <strong>Dapeng Hu</strong>,
              <a href="https://scholar.google.com/citations?hl=en&user=zuYIUJEAAAAJ">Yifan Zhang</a>,	
              <a href="https://scholar.google.com/citations?user=eY8i-mQAAAAJ&hl=en">Jian Liang</a>,
              <a href="https://scholar.google.com/citations?user=Q8iay0gAAAAJ&hl=en">Jiashi Feng</a>

          <br>
              <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021.
          <br>
              <a href="https://arxiv.org/pdf/2106.05001.pdf" target="_blank">arXiv</a>
          <br>
          <br>
              <p></p>
              <p>We proposed CCVR, a simple and universal classifier calibration algorithm for federated learning.</p>
          </td>
      </tr>
      </tbody></table>
<!-- next paper -->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="two" id='impact_image'><img src='images/coretuning.png'></div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
          <a href=https://proceedings.neurips.cc/paper/2021/hash/fa14d4fe2f19414de3ebd9f63d5c0169-Abstract.html>
          <papertitle>Unleashing the Power of Contrastive Self-Supervised Visual Models via Contrast-Regularized Fine-Tuning</papertitle> </a>
      <br>
          <a href="https://scholar.google.com/citations?hl=en&user=zuYIUJEAAAAJ">Yifan Zhang</a>,
          <a href="https://scholar.google.com/citations?user=ErEL3bgAAAAJ&hl=en">Bryan Hooi</a>,		
          <strong>Dapeng Hu</strong>,
          <a href="https://scholar.google.com/citations?user=eY8i-mQAAAAJ&hl=en">Jian Liang</a>,
          <a href="https://scholar.google.com/citations?user=Q8iay0gAAAAJ&hl=en">Jiashi Feng</a>

      <br>
          <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021.
      <br>
          <a href="https://arxiv.org/pdf/2102.06605.pdf" target="_blank">arXiv</a> &nbsp/&nbsp
          <a href="https://github.com/Vanint/Core-tuning" target="_blank">code</a>

      <br>
      <br>
          <p></p>
          <p>We proposed a theoretically and empirically promising Core-tuning method for fine-tuning contrastive self-supervised models.</p>
      </td>
  </tr>
  </tbody></table>
<!-- next paper -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="two" id='impact_image'><img src='images/atdoc.png'></div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <a href=https://openaccess.thecvf.com/content/CVPR2021/papers/Liang_Domain_Adaptation_With_Auxiliary_Target_Domain-Oriented_Classifier_CVPR_2021_paper.pdf>
              <papertitle>Domain Adaptation with Auxiliary Target Domain-Oriented Classifier</papertitle> </a>
          <br>
              <a href="https://scholar.google.com/citations?user=eY8i-mQAAAAJ&hl=en">Jian Liang</a>,
              <strong>Dapeng Hu</strong>,
              <a href="https://scholar.google.com/citations?user=Q8iay0gAAAAJ&hl=en">Jiashi Feng</a>
          <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021.
          <br>
              <a href="https://arxiv.org/pdf/2007.04171.pdf" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href="https://github.com/tim-learn/ATDOC" target="_blank">code</a>
          <br>
          <br>
              <p></p>
              <p>We proposed ATDOC, a simple yet effective framework to combat classifier bias that provided a novel perspective addressing domain shift.</p>
          </td>
      </tr>
      </tbody></table>
<!-- next paper -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="two" id='impact_image'><img src='images/shot.png'></div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <a href=http://proceedings.mlr.press/v119/liang20a/liang20a.pdf>
              <papertitle>Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation</papertitle> </a>
          <br>
              <a href="https://scholar.google.com/citations?user=eY8i-mQAAAAJ&hl=en">Jian Liang</a>,
              <strong>Dapeng Hu</strong>,
              <a href="https://scholar.google.com/citations?user=Q8iay0gAAAAJ&hl=en">Jiashi Feng</a>
          <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2020.
          <br>
              <a href="https://arxiv.org/pdf/2002.08546.pdf" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href="https://github.com/tim-learn/SHOT" target="_blank">code</a>
          <br>
          <br>
              <p></p>
              <p>We were among the first to look into a practical unsupervised domain adaptation setting called "source-free" DA and proposed a simple yet generic representation learning framework named SHOT.</p>
          </td>
      </tr>
      </tbody></table>
<!-- next paper -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="two" id='impact_image'><img src='images/ba3us.png'></div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <a href=https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560120.pdf>
              <papertitle>A Balanced and Uncertainty-aware Approach for Partial Domain Adaptation</papertitle> </a>
          <br>
              <a href="https://scholar.google.com/citations?user=eY8i-mQAAAAJ&hl=en">Jian Liang</a>,
              <a href="http://59.108.48.34/tiki/news/news.php?id=EGwangyunbo">Yunbo Wang</a>,
              <strong>Dapeng Hu</strong>,
              <a href="https://scholar.google.com/citations?user=ayrg9AUAAAAJ&hl=en">Ran He</a>,	
              <a href="https://scholar.google.com/citations?user=Q8iay0gAAAAJ&hl=en">Jiashi Feng</a>
          <br>
              <em>European Conference on Computer Vision (ECCV)</em>, 2020.
          <br>
              <a href="https://arxiv.org/pdf/2003.02541.pdf" target="_blank">arXiv</a> &nbsp/&nbsp
              <a href="https://github.com/tim-learn/BA3US" target="_blank">code</a>
          <br>
          <br>
              <p></p>
              <p> We tackled partial domain adaptation by augmenting the target domain and transforming it into an unsupervised domain adaptation problem</p>
          </td>
      </tr>
      </tbody></table>

<!-- end paper list -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Professional Service</heading>
          <p>
            <strong>Journal Reviewer</strong>: <a href="https://openreview.net/group?id=TMLR">TMLR</a> 2022, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a> 2022, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=69">TKDE</a> 2022
          </p>

          <p>
            <strong>Conference Reviewer</strong>: <a href="https://icml.cc/">ICML</a> 2021-2023, <a href="https://nips.cc/">NeurIPS</a> 2021-2023, <a href="https://cvpr2022.thecvf.com/">CVPR</a> 2022-2023, <a href="https://iccv2023.thecvf.com/">ICCV</a> 2023, <a href="https://iclr.cc/">ICLR</a> 2022-2024
          </p>

          <br>
            <p><a href="https://nusmods.com/modules/EE6934/deep-learning-advanced">Head TA, EE6934: Deep Learning (Advanced), 2020 Spring</a></p>
            <p><a href="https://nusmods.com/modules/EE5934/deep-learning">Head TA, EE5934: Deep Learning, 2020 Spring</a></p>
            <p><a href="https://nusmods.com/modules/EE4704/image-processing-and-analysis">TA, EE4704: Image Processing and Analysis, 2019 Fall</a></p>
            <p><a href="https://nusmods.com/modules/EE2028/microcontroller-programming-and-interfacing">TA, EE2028: Microcontroller Programming and Interfacing, 2019 Fall</a></p>
        </td>
      </tr>
    </tbody></table>

	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Awards and Honors</heading>
          <ul>
          <li><p><a href="https://icml.cc/Conferences/2022/Reviewers">ICML Outstanding Reviewers (Top 10%)</a> (2022)</p></li>
          <li><p><a href="https://fass.nus.edu.sg/prospective-students/graduate/research/scholarships/nus-research-scholarship/">NUS Research Scholarship</a> (2019-2023)</p></li>
          <li><p>Outstanding Graduate of Nanjing University (2017)</p></li>
          <li><p>Merit Student of Jiangsu Province (2013)</p></li>
          </ul>
			  </td>
		  </tr>
    </tbody></table>
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Credit</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>

